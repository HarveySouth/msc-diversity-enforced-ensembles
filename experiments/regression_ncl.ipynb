{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # # local\n",
        "project_directory = \"../\"\n",
        "\n",
        "\n",
        "# # # # colab\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "# project_directory = \"/content/drive/MyDrive/colab_working_directory/diversity-enforced-ensembles/\"\n",
        "# !pip install cached-property"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "5aA0f_vvrEvZ"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# allow import of decompose locally\n",
        "import sys\n",
        "sys.path.append(project_directory + 'src/')\n",
        "\n",
        "from decompose import SquaredLoss\n",
        "import bvdlib\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch.func import stack_module_state\n",
        "from torch.func import functional_call\n",
        "from torch import vmap\n",
        "import copy\n",
        "\n",
        "from bvdlib.trial import Trial\n",
        "from numpy.random import choice\n",
        "from numpy.random import seed\n",
        "from numpy import array\n",
        "import numpy as np\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "save_path = project_directory + \"experiments/results/NCL_Regression_CaliforniaHousing.pkl\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f3_NR9JfrHQ3",
        "outputId": "f0a64c0b-dcc9-4ae4-82b5-35cb12d9962e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "x shape (20640, 8)\n",
            "y shape (20640,)\n"
          ]
        }
      ],
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "housing = fetch_california_housing()\n",
        "\n",
        "x = housing['data']\n",
        "y = housing['target']\n",
        "\n",
        "print(\"x shape\", x.shape)\n",
        "print(\"y shape\", y.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "-D2DQbOoshI7"
      },
      "outputs": [],
      "source": [
        "n_trials = 100\n",
        "trial_space = np.arange(1,21) # Test estimators from 1 to 20\n",
        "data_percentage_training = int(0.8 * len(y))\n",
        "num_training = int(0.8 * data_percentage_training)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "_1tbt-7zsosi"
      },
      "outputs": [],
      "source": [
        "def init_layer(layer, generator):\n",
        "    torch.nn.init.xavier_uniform_(layer.weight, generator=generator)\n",
        "    layer.bias.data.fill_(0.01)\n",
        "\n",
        "class SimpleMLP(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, generator):\n",
        "        super(SimpleMLP, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "        init_layer(self.fc1, generator)\n",
        "\n",
        "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        init_layer(self.fc2, generator)\n",
        "\n",
        "        self.fc3 = nn.Linear(hidden_dim, output_dim)\n",
        "        init_layer(self.fc3, generator)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = self.fc1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.fc3(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "8W_8KizJQCm4"
      },
      "outputs": [],
      "source": [
        "class trial_dataset(torch.utils.data.Dataset):\n",
        "  def __init__(self, x, y, device):\n",
        "    self.x = torch.tensor(x).type(torch.float).to(device)\n",
        "    self.y = torch.tensor(y).type(torch.float).to(device)\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.x)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return self.x[idx], self.y[idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "NOX-IahFUk1i"
      },
      "outputs": [],
      "source": [
        "experiment_seed = 0\n",
        "np.random.seed(experiment_seed)\n",
        "rng = np.random.default_rng()\n",
        "shuffled_indices = rng.permutation(len(y))\n",
        "\n",
        "train_indices = shuffled_indices[:data_percentage_training]\n",
        "test_indices = shuffled_indices[data_percentage_training:]\n",
        "train_data = x[train_indices, :]\n",
        "train_labels = y[train_indices]\n",
        "test_data = x[test_indices, :]\n",
        "test_labels = y[test_indices]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "zRKaNa0auP7_"
      },
      "outputs": [],
      "source": [
        "def forward_through_metamodel(params, buffers, x):\n",
        "        return functional_call(meta_model, (params, buffers), (x,))\n",
        "\n",
        "def torch_MSE_combiner(ensemble_output):\n",
        "    return torch.mean(ensemble_output, axis=0)\n",
        "\n",
        "combiner_rule = torch_MSE_combiner\n",
        "\n",
        "def ens_forward(input, ensemble):\n",
        "\n",
        "  params, buffers = stack_module_state(nn.ModuleList(ensemble))\n",
        "\n",
        "  member_output = vmap(forward_through_metamodel)(params, buffers, input.repeat(len(ensemble), 1, 1))\n",
        "\n",
        "  ensemble_output = combiner_rule(member_output)\n",
        "  return ensemble_output, member_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ResultsObject(object):\n",
        "    \"\"\"\n",
        "    Results from BVDExperiment are stored in ResultsObject instances.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    parameter_name : str\n",
        "        name of parameter that is varied over the course of the experiment\n",
        "    parameter_values : list\n",
        "        list of values that the varied parameter takes\n",
        "    loss_func : str\n",
        "        name of the loss function used for the decompose\n",
        "    n_test_splits : int\n",
        "        Number of separate folds unseen data is split into. Default is 2, with the first being the train split and the\n",
        "        second being test\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    ensemble_risk : ndarray of shape (n_parameter_values, n_test_splits)\n",
        "        The risk of the ensemble for each parameter value and test split\n",
        "    ensemble_bias: ndarray of shape (n_parameter_values, n_test_splits)\n",
        "        The biasof the ensemble for each paramter value and test split\n",
        "    ensemble_variance: ndarray of shape (n_parameter_values, n_test_splits)\n",
        "        The varianceof the ensemble for each parameter value and test split\n",
        "    average_bias : ndarray of shape (n_parameter_values, n_test_splits)\n",
        "        The average bias of the ensemble members for each parmater value and test split\n",
        "    average_variance : ndarray of shape (n_parameter_values, n_test_splits)\n",
        "        The average variance of the ensemble members for each parmater value and test split\n",
        "    diversity : ndarray of shape (n_parameter_values, n_test_splits)\n",
        "        The diversity for each parameter value and test split\n",
        "    test_error : ndarray of shape (n_parameter_values, n_test_splits)\n",
        "        The test error of the ensemble for each parameter value and test split\n",
        "    train_error : ndarray of shape (n_parameter_values)\n",
        "        The train error of the ensemble for each parameter value (each ensemble is evaluated only on data that it has seen\n",
        "        during training.\n",
        "    member_test_error : ndarray of shape (n_parameter_values, n_test_splits)\n",
        "        The average test error of the ensemble for each parameter value and test split\n",
        "    member_train_error : ndarray of shape (n_parameter_values)\n",
        "        The average train error of the ensemble for each parameter value. Members are evaluated on data that was seen by the\n",
        "        ensemble during training; there may be examples that were seen by the ensemble but not the individual member\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, parameter_name, parameter_values, loss_func,\n",
        "                 n_test_splits):\n",
        "        n_parameter_values = len(parameter_values)\n",
        "        self.loss_func = loss_func\n",
        "        self.parameter_name = parameter_name\n",
        "        self.parameter_values = parameter_values\n",
        "        self.n_test_splits = n_test_splits\n",
        "        self.ensemble_risk = np.zeros((n_parameter_values, n_test_splits))\n",
        "        self.ensemble_bias = np.zeros((n_parameter_values, n_test_splits))\n",
        "        self.ensemble_variance = np.zeros((n_parameter_values, n_test_splits))\n",
        "        self.average_bias = np.zeros((n_parameter_values, n_test_splits))\n",
        "        self.average_variance = np.zeros((n_parameter_values, n_test_splits))\n",
        "        self.diversity = np.zeros((n_parameter_values, n_test_splits))\n",
        "        self.test_error = np.zeros((n_parameter_values, n_test_splits))\n",
        "        self.train_error = np.zeros((n_parameter_values))\n",
        "        self.member_test_error = np.zeros((n_parameter_values, n_test_splits))\n",
        "        self.member_train_error = np.zeros((n_parameter_values))\n",
        "\n",
        "\n",
        "    def update_results(self, decomp, param_idx, errors, split_idx=0, sample_weight=None):\n",
        "        \"\"\"\n",
        "        Function used to update ResultsObject for a new parameter using Decomposition object and list of train/test errors\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        decomp : Decomposition\n",
        "            Decomposition object for the experiment\n",
        "        param_idx : int\n",
        "            The index of the current parameter in the parameter_values\n",
        "        errors : list of floats\n",
        "            List containing (in order):\n",
        "                Training error averaged over all runs of the experiment\n",
        "                Test error averaged over all runs of the experiment\n",
        "                (optional)\n",
        "                Average member train error\n",
        "                Average member test error\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        None\n",
        "\n",
        "        \"\"\"\n",
        "        self.train_error[param_idx] = errors[0]  # overall error\n",
        "        self.test_error[param_idx, split_idx] = errors[1][split_idx]  # overall error\n",
        "\n",
        "        if len(errors) == 4:\n",
        "            self.member_train_error[param_idx] = errors[2]  # avg member error\n",
        "            self.member_test_error[param_idx, split_idx] = errors[3][split_idx]  # avg member error\n",
        "\n",
        "        # We can tell if we have per trial test erorrs by checking if the length of the errors list is odd\n",
        "        if len(errors) % 2 == 1:\n",
        "            if not hasattr(self, \"per_trial_test_errors\"):\n",
        "                self.per_trial_test_errors = np.zeros((len(self.parameter_values),\n",
        "                                                       errors[-1].shape[0],\n",
        "                                                       self.n_test_splits))\n",
        "            # This also doesn't feel great, is it already filled?\n",
        "            self.per_trial_test_errors[param_idx, :, split_idx] = errors[-1][:, split_idx]\n",
        "\n",
        "\n",
        "        self.ensemble_bias[param_idx, split_idx] = np.average(decomp.ensemble_bias,\n",
        "                                                              weights=sample_weight)\n",
        "\n",
        "        self.ensemble_variance[param_idx, split_idx] = np.average(decomp.ensemble_variance,\n",
        "                                                                                  weights=sample_weight)\n",
        "\n",
        "        self.average_bias[param_idx, split_idx] = np.average(decomp.average_bias,\n",
        "                                                             weights=sample_weight)\n",
        "\n",
        "        self.average_variance[param_idx, split_idx] = np.average(decomp.average_variance,\n",
        "                                                                 weights=sample_weight)\n",
        "\n",
        "        self.diversity[param_idx, split_idx] = np.average(decomp.diversity, weights=sample_weight)\n",
        "\n",
        "        self.ensemble_risk[param_idx, split_idx] = np.average(decomp.expected_ensemble_loss,\n",
        "                                                              weights=sample_weight)\n",
        "        # logger.debug(f\"Update Summary {param_idx},{split_idx}--\"\n",
        "        #              f\"ensemble bias: {self.ensemble_bias[param_idx, split_idx]},\"\n",
        "        #              f\" ensemble variance: {self.ensemble_variance[param_idx, split_idx]},\"\n",
        "        #              f\" average bias: {self.average_bias[param_idx, split_idx]},\"\n",
        "        #              f\"average variance: {self.average_variance[param_idx, split_idx]}, \"\n",
        "        #              f\"diversity: {self.diversity[param_idx, split_idx]},\"\n",
        "        #              f\" ensemble risk{self.ensemble_risk[param_idx, split_idx]},\"\n",
        "        #              f\" test error:{self.test_error[param_idx, split_idx]},\"\n",
        "        #              f\" train error{self.train_error[param_idx]}\")\n",
        "    def save_results(self, file_path):\n",
        "            \"\"\"\n",
        "            Saves results object to pickle file for later use\n",
        "\n",
        "            Parameters\n",
        "            ----------\n",
        "            file_path : str\n",
        "                name of file (inlcuding directory) in which results are toe be stored\n",
        "\n",
        "            Returns\n",
        "            -------\n",
        "            None\n",
        "\n",
        "            \"\"\"\n",
        "            with open(file_path, \"wb+\") as file_:\n",
        "                pickle.dump(self, file_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "trial_space = np.arange(0,15) / 10\n",
        "decomp_fn = SquaredLoss\n",
        "seed = 0\n",
        "criterion = torch.nn.MSELoss()\n",
        "epoch_n=7\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "\n",
        "torch_generator = torch.manual_seed(0)\n",
        "\n",
        "test_dset = trial_dataset(test_data, test_labels, device)\n",
        "\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(test_dset, batch_size = 258, shuffle=False)\n",
        "\n",
        "train_dset = trial_dataset(train_data, train_labels, device)\n",
        "train_dloader = torch.utils.data.DataLoader(train_dset, batch_size = 128, shuffle=True, generator=torch_generator)\n",
        "train_unshuffled_dloader = torch.utils.data.DataLoader(train_dset, batch_size = 258, shuffle=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[19], line 70\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;66;03m# save results\u001b[39;00m\n\u001b[1;32m     67\u001b[0m study \u001b[38;5;241m=\u001b[39m bvdlib\u001b[38;5;241m.\u001b[39mNCL_Study(trial_space, train_data, train_labels, test_data, test_labels, \n\u001b[1;32m     68\u001b[0m                      num_training, n_trials, decomp_fn, epoch_n)\n\u001b[0;32m---> 70\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mstudy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_trials\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial_run\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m results\u001b[38;5;241m.\u001b[39msave_results(save_path)\n",
            "File \u001b[0;32m~/work/EDMP/diversity-enforced-ensembles/experiments/../src/bvdlib/ncl_study.py:63\u001b[0m, in \u001b[0;36mNCL_Study.run_trials\u001b[0;34m(self, trial_function)\u001b[0m\n\u001b[1;32m     59\u001b[0m trial \u001b[38;5;241m=\u001b[39m Trial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_data[trial_idx], \n\u001b[1;32m     60\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_labels[trial_idx],\n\u001b[1;32m     61\u001b[0m             param)\n\u001b[1;32m     62\u001b[0m \u001b[38;5;66;03m#run trial\u001b[39;00m\n\u001b[0;32m---> 63\u001b[0m trial_results, train_losses \u001b[38;5;241m=\u001b[39m \u001b[43mtrial_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepoch_n):\n\u001b[1;32m     67\u001b[0m     total_train_loss_epoch[epoch] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_trials) \u001b[38;5;241m*\u001b[39m train_losses[epoch]\n",
            "Cell \u001b[0;32mIn[19], line 50\u001b[0m, in \u001b[0;36mtrial_run\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m     47\u001b[0m         ens_grad_output \u001b[38;5;241m=\u001b[39m combiner_rule(member_grad_output)\n\u001b[1;32m     49\u001b[0m         member_loss \u001b[38;5;241m=\u001b[39m (criterion(member_pred, by\u001b[38;5;241m.\u001b[39munsqueeze(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)) \u001b[38;5;241m+\u001b[39m ((lambda_) \u001b[38;5;241m*\u001b[39m criterion(member_pred, ens_grad_output)))\n\u001b[0;32m---> 50\u001b[0m         \u001b[43mmember_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m         optims[i]\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
            "File \u001b[0;32m/opt/miniconda3/envs/ensemble-diversity/lib/python3.12/site-packages/torch/_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    520\u001b[0m     )\n\u001b[0;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/opt/miniconda3/envs/ensemble-diversity/lib/python3.12/site-packages/torch/autograd/__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/opt/miniconda3/envs/ensemble-diversity/lib/python3.12/site-packages/torch/autograd/graph.py:768\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    766\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    767\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 768\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    769\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    770\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    771\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    772\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "# define a trial\n",
        "def trial_run(trial):\n",
        "\n",
        "    epoch_results = []\n",
        "    epoch_train_losses = []\n",
        "\n",
        "    trial_x, trial_y = trial.get_data\n",
        "\n",
        "    trial_dset = trial_dataset(trial_x, trial_y, device)\n",
        "    trial_dloader = torch.utils.data.DataLoader(trial_dset, batch_size = 128, shuffle=True, generator=torch_generator)\n",
        "    trial_unshuffled_dloader = torch.utils.data.DataLoader(trial_dset, batch_size = 258, shuffle=False)\n",
        "\n",
        "    # init model\n",
        "    ensemble = []\n",
        "    optims = []\n",
        "    losses = []\n",
        "\n",
        "    n_estimators = 11\n",
        "    for member_n in range(n_estimators):\n",
        "        ensemble.append(SimpleMLP(len(trial_x[0]), 24, 1, torch_generator).to(device))\n",
        "        optims.append(torch.optim.Adam(ensemble[member_n].parameters()))\n",
        "        losses.append(None)\n",
        "\n",
        "    global meta_model \n",
        "    meta_model = copy.deepcopy(ensemble[0]).to('meta')\n",
        "    \n",
        "    lambda_ = trial.get_param\n",
        "\n",
        "\n",
        "    for epoch in range(epoch_n):\n",
        "\n",
        "        trial_results_array = np.zeros((n_estimators, len(test_data)))\n",
        "\n",
        "        for batch_idx, batch in enumerate(trial_dloader):\n",
        "            bx, by = batch\n",
        "\n",
        "            with torch.no_grad():\n",
        "                ensemble_output, member_output = ens_forward(bx, ensemble)\n",
        "\n",
        "            member_output = member_output.detach()\n",
        "\n",
        "            for i, member in enumerate(ensemble):\n",
        "\n",
        "                optims[i].zero_grad()\n",
        "                member_pred = member(bx) #predict\n",
        "                member_grad_output = torch.cat((member_output[:i], member_pred.unsqueeze(dim=0), member_output[i+1:]))\n",
        "                ens_grad_output = combiner_rule(member_grad_output)\n",
        "\n",
        "                member_loss = (criterion(member_pred, by.unsqueeze(dim=-1)) - ((lambda_) * criterion(member_pred, ens_grad_output)))\n",
        "                member_loss.backward()\n",
        "                optims[i].step()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for i, member in enumerate(ensemble):\n",
        "                train_preds = member(torch.tensor(trial_x).type(torch.float).to(device))\n",
        "                epoch_train_losses.append(criterion(train_preds.to(device), torch.tensor(trial_y).unsqueeze(dim=-1).type(torch.float).to(device)).cpu())\n",
        "                member_preds = member(torch.tensor(test_data).type(torch.float).to(device))\n",
        "                trial_results_array[i, :] = member_preds.cpu().squeeze()\n",
        "        \n",
        "        epoch_results.append(np.array(trial_results_array)) \n",
        "\n",
        "    return epoch_results, epoch_train_losses \n",
        "\n",
        "# save results\n",
        "    \n",
        "\n",
        "study = bvdlib.NCL_Study(trial_space, train_data, train_labels, test_data, test_labels, \n",
        "                     num_training, n_trials, decomp_fn, epoch_n)\n",
        "\n",
        "results = study.run_trials(trial_run)\n",
        "\n",
        "\n",
        "results.save_results(save_path)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
